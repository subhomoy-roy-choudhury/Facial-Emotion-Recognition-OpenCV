{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "def shot(IMAGE):\n",
    "    camera_port = 0\n",
    "    camera = cv2.VideoCapture(camera_port)\n",
    "    time.sleep(0.1)  # If you don't wait, the image will be dark\n",
    "    return_value, image = camera.read()\n",
    "    cv2.imwrite(IMAGE, image)\n",
    "    del(camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import MobileNet\n",
    "from keras.models import Sequential,Model \n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D,MaxPooling2D,ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# MobileNet is designed to work with images of dim 224,224\n",
    "img_rows,img_cols = 224,224\n",
    "\n",
    "MobileNet = MobileNet(weights='imagenet',include_top=False,input_shape=(img_rows,img_cols,3))\n",
    "\n",
    "# Here we freeze the last 4 layers\n",
    "# Layers are set to trainable as True by default\n",
    "\n",
    "for layer in MobileNet.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Let's print our layers\n",
    "for (i,layer) in enumerate(MobileNet.layers):\n",
    "    print(str(i),layer.__class__.__name__,layer.trainable)\n",
    "\n",
    "def addTopModelMobileNet(bottom_model, num_classes):\n",
    "    \"\"\"creates the top or head of the model that will be \n",
    "    placed ontop of the bottom layers\"\"\"\n",
    "\n",
    "    top_model = bottom_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "    top_model = Dense(1024,activation='relu')(top_model)\n",
    "    \n",
    "    top_model = Dense(1024,activation='relu')(top_model)\n",
    "    \n",
    "    top_model = Dense(512,activation='relu')(top_model)\n",
    "    \n",
    "    top_model = Dense(num_classes,activation='softmax')(top_model)\n",
    "\n",
    "    return top_model\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "FC_Head = addTopModelMobileNet(MobileNet, num_classes)\n",
    "\n",
    "model = Model(inputs = MobileNet.input, outputs = FC_Head)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "train_data_dir = '/Users/durgeshthakur/Deep Learning Stuff/Emotion Classification/fer2013/train'\n",
    "validation_data_dir = '/Users/durgeshthakur/Deep Learning Stuff/Emotion Classification/fer2013/validation'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "                    rotation_range=30,\n",
    "                    width_shift_range=0.3,\n",
    "                    height_shift_range=0.3,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest'\n",
    "                                   )\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                        train_data_dir,\n",
    "                        target_size = (img_rows,img_cols),\n",
    "                        batch_size = batch_size,\n",
    "                        class_mode = 'categorical'\n",
    "                        )\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "                            validation_data_dir,\n",
    "                            target_size=(img_rows,img_cols),\n",
    "                            batch_size=batch_size,\n",
    "                            class_mode='categorical')\n",
    "\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "                             'emotion_face_mobilNet.h5',\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(\n",
    "                          monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=10,\n",
    "                          verbose=1,restore_best_weights=True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=5, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.2, \n",
    "                                            min_lr=0.0001)\n",
    "\n",
    "callbacks = [earlystop,checkpoint,learning_rate_reduction]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "nb_train_samples = 24176\n",
    "nb_validation_samples = 3006\n",
    "\n",
    "epochs = 25\n",
    "\n",
    "history = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=nb_train_samples//batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=nb_validation_samples//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
    "classifier =load_model('./Emotion_Detection.h5')\n",
    "\n",
    "class_labels = ['Angry','Happy','Neutral','Sad','Surprise']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = cap.read()\n",
    "    labels = []\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray,1.3,5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h,x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "        if np.sum([roi_gray])!=0:\n",
    "            roi = roi_gray.astype('float')/255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi,axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "\n",
    "            preds = classifier.predict(roi)[0]\n",
    "            print(\"\\nprediction = \",preds)\n",
    "            label=class_labels[preds.argmax()]\n",
    "            print(\"\\nprediction max = \",preds.argmax())\n",
    "            print(\"\\nlabel = \",label)\n",
    "            label_position = (x,y)\n",
    "            cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "        else:\n",
    "            cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "        print(\"\\n\\n\")\n",
    "    cv2.imshow('Emotion Detector',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## emotion_func.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os \n",
    "from keras.models import load_model\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing import image\n",
    "from shot import shot\n",
    "\n",
    "# recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "# recognizer.read('trainer/trainer.yml')\n",
    "def emotion():\n",
    "    shot('opencv.png')\n",
    "    classifier =load_model('./Emotion_Detection.h5')\n",
    "    cascadePath = \"haarcascade_frontalface_default.xml\"\n",
    "    faceCascade = cv2.CascadeClassifier(cascadePath);\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    #iniciate id counter\n",
    "    id = 0\n",
    "\n",
    "    # Emotions related to ids: example ==> Anger: id=0,  etc\n",
    "    names = ['Angry','Happy','Neutral','Sad','Surprise']\n",
    "\n",
    "    img = cv2.imread(\"opencv.png\")\n",
    "\n",
    "\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale( \n",
    "        gray,\n",
    "        scaleFactor = 1.2,\n",
    "        minNeighbors = 5,\n",
    "        )\n",
    "\n",
    "    for(x,y,w,h) in faces:\n",
    "\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "        roi_gray = gray[y:y+h,x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "        if np.sum([roi_gray])!=0:\n",
    "                roi = roi_gray.astype('float')/255.0\n",
    "                roi = img_to_array(roi)\n",
    "                roi = np.expand_dims(roi,axis=0)\n",
    "\n",
    "                preds = classifier.predict(roi)[0]\n",
    "                print(\"\\nprediction = \",preds)\n",
    "                label=names[preds.argmax()]\n",
    "                print(\"\\nprediction max = \",preds.argmax())\n",
    "                print(\"\\nlabel = \",label)\n",
    "                label_position = (x,y)\n",
    "                cv2.putText(img,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "                cv2.imwrite(\"opencv_test.png\",img)\n",
    "                print(\"\\n [INFO] Done detecting and Image is saved\")\n",
    "                return label \n",
    "        else :\n",
    "            print('not found')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
